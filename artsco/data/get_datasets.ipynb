{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Sent Environment Variables\n",
    "print(\"Environment variables set successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Personas: 339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/339 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from utils_apicall import get_response_from_openai, extract_xml_field\n",
    "\n",
    "file_path = \"persona/characters.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Number of Unique Personas:\", len(list(data.keys())))\n",
    "\n",
    "relevant_keys = [\"character_name\", \"gender\", \"mbti\", \"biography\"]\n",
    "bio_ids = list(data.keys())\n",
    "clean_bios = []\n",
    "for idx in bio_ids:\n",
    "    clean_bios.append({key: val  for key, val in data[idx].items() if key in relevant_keys})\n",
    "\n",
    "OUTPUT_FIELDS = [\"general\", \"shopping_preferences\", \"political_preferences\", \"socialmedia_preferences\", \"economic_status\"]\n",
    "XML_INSTRUCTIONS = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"When you reply, enclose your answer inside \"\n",
    "    \"<general> … </general>, \"\n",
    "    \"<shopping_preferences> … </shopping_preferences>, \"\n",
    "    \"<political_preferences> … </political_preferences>, \"\n",
    "    \"<socialmedia_preferences> … </socialmedia_preferences>. \"\n",
    "    \"<economic_status> … </economic_status>. \"\n",
    "    \"Do not output anything else.\"\n",
    ")\n",
    "\n",
    "BIO_INSTRUCTIONS = lambda bio:  (\n",
    "        \"Write a character bio set in the present day, based on the provided information. \"\n",
    "        \"The bio must be structured into the following sections:\\n\\n\"\n",
    "        \"### General\\n\"\n",
    "        \"Provide a brief overview of the person’s background, personality, and daily life.\\n\\n\"\n",
    "        \"### Shopping Preferences\\n\"\n",
    "        \"Describe the types of products they prefer to purchase and the reasons behind their choices.\\n\\n\"\n",
    "        \"### Political Preferences\\n\"\n",
    "        \"Describe the kinds of political candidates they are most likely to support.\\n\\n\"\n",
    "        \"### Social Media Preferences\\n\"\n",
    "        \"Describe the types of posts they are most likely to 'like' or engage with.\\n\\n\"\n",
    "        \"### Economic Status\\n\"\n",
    "        \"Describe their current monthly income.\\n\\n\"\n",
    "        f\"Information: {bio}\"\n",
    "        \"Make sure the bio is written as a realistic present-day persona, even if the provided information is about an imaginary character in the past or the future.\"\n",
    "\n",
    "    )\n",
    "\n",
    "def process_bio(bio):\n",
    "    prompt = BIO_INSTRUCTIONS(bio)\n",
    "    res = get_response_from_openai(query=prompt, model=\"gpt-4o\", XML_INSTRUCTIONS=XML_INSTRUCTIONS)\n",
    "    res = {output_field: extract_xml_field(res, output_field) for output_field in OUTPUT_FIELDS}\n",
    "    return res\n",
    "\n",
    "RES = []\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:  # tune workers to API rate limits\n",
    "    futures = [executor.submit(process_bio, bio) for bio in clean_bios]\n",
    "    for f in tqdm(as_completed(futures), total=len(futures)):\n",
    "        RES.append(f.result())\n",
    "\n",
    "random.shuffle(RES)\n",
    "file_path = \"persona/all.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(RES, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:03<00:00, 6607.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Products: 2197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Electronics\", trust_remote_code=True)\n",
    "# print(dataset[\"full\"][0])\n",
    "\n",
    "data_length = len(dataset[\"full\"])\n",
    "\n",
    "data_clean = []\n",
    "\n",
    "for i in tqdm(range(25000)):\n",
    "    curr = dataset[\"full\"][i]\n",
    "    desc = '\\n-'.join(curr[\"description\"])\n",
    "    categories = curr[\"categories\"]\n",
    "    title = curr[\"title\"]\n",
    "    if title and desc and categories:\n",
    "        if (len(desc) >= 500) and (len(desc) <= 750) :\n",
    "            data_clean.append({\"title\": title, \"categories\": categories, \"description\": desc})\n",
    "\n",
    "print(\"Number of Unique Products:\", len(data_clean))\n",
    "\n",
    "random.shuffle(data_clean)\n",
    "\n",
    "train_split = data_clean[:1024]\n",
    "test_split = data_clean[1024:2048]\n",
    "\n",
    "\n",
    "file_path = \"task_sales/train.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(train_split, file, indent=4, ensure_ascii=False)\n",
    "file_path = \"task_sales/test.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(test_split, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Candidates: 2050\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "path = \"task_elections/biographical_narratives.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "df['bio_length'] = df['biography_text'].astype(str).apply(lambda x: len(x.split()))\n",
    "df = df[(85 <= df['bio_length']) &  (df['bio_length'] < 350)]\n",
    "relevant_columns = [\"candidate_webname\", \"cand_party\", \"biography_text\"]\n",
    "data = df[relevant_columns].to_dict(orient=\"records\")\n",
    "\n",
    "print(\"Number of Unique Candidates:\", len(data))\n",
    "random.shuffle(data)\n",
    "trian_split = data[:1024]\n",
    "test_split = data[1024:2048]\n",
    "\n",
    "file_path = \"task_elections/train.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(trian_split, file, indent=4, ensure_ascii=False)\n",
    "file_path = \"task_elections/test.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(test_split, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by=\"bio_length\")[relevant_columns].iloc[0]['biography_text']\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(df['bio_length'], bins=20, edgecolor='black')\n",
    "# plt.title('Histogram of Biography Lengths (words)')\n",
    "# plt.xlabel('Biography Length (words)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Posts: 6039\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json \n",
    "\n",
    "# dataset = load_dataset(\"EmergentMethods/AskNews-NER-v0\", split=\"train\", trust_remote_code=True) # Not used (multimodal_)\n",
    "\n",
    "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "fds = ds[\"train\"].filter(lambda x: 100 <= len(x['article'].split()) < 200)\n",
    "\n",
    "print(\"Number of Unique Posts:\", len(fds))\n",
    "\n",
    "shuffled_ds = fds.shuffle().to_list()\n",
    "trian_split = shuffled_ds[:1024]\n",
    "test_split = shuffled_ds[1024:2048]\n",
    "\n",
    "file_path = \"task_sm/train.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(trian_split, file, indent=4, ensure_ascii=False)\n",
    "file_path = \"task_sm/test.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(test_split, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counts = [len(article.split()) for article in fds['article']]\n",
    "\n",
    "# # Plot histogram\n",
    "# plt.hist(word_counts, bins=50, edgecolor='black')\n",
    "# plt.xlabel(\"Article length (words)\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of Article Lengths\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfds-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
