{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Sent Environment Variables\n",
    "print(\"Environment variables set successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from transformers import  AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "# from artsco.utils import process_dataset, MODELS, apply_chat_template\n",
    "\n",
    "from artsco.voter.voters import Voters\n",
    "from artsco.voter.utils import load_persona100\n",
    "\n",
    "# CURRENT_MODELS = [\"Qwen/Qwen3-8B\",  \"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen3-14B\"]\n",
    "QWEN_MODEL_NAMES = [\"Qwen/Qwen3-8B\"] #, \"Qwen/Qwen3-32B\"]\n",
    "LLAMA_MODEL_NAMES = [\"meta-llama/Llama-3.1-8B-Instruct\"] #, \"meta-llama/Llama-3.1-70B-Instruct\"]\n",
    "OPENAI_MODEL_NAMES = [\"openai/gpt-oss-20b\"]\n",
    "MODELS = QWEN_MODEL_NAMES + LLAMA_MODEL_NAMES \n",
    "TASKS = [\"task_elections\", \"task_sales\" , \"task_sm\"]\n",
    "SPLITS = [\"train\"]\n",
    "METHODS =[\"base\", \"rft\", \"tfb\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Second Stage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\"task_elections\", \"task_sales\" , \"task_sm\"]\n",
    "\n",
    "results_path_root = \"artsco/res\"\n",
    "split = \"test\"\n",
    "datasets = {task: {model: {method: load_dataset(\"json\", data_files=os.path.join(results_path_root,  task, model, method, f\"{split}_step2.json\"))['train'] for method in METHODS} for model in MODELS} for task in TASKS} \n",
    "\n",
    "num_voters = 20\n",
    "bios = load_persona100()[:num_voters]\n",
    "\n",
    "# voter_votes, voter_thinks = voters.get_votes_list(player_candidates)\n",
    "\n",
    "\n",
    "all_candidates0 = {task: {model: {method: [] for method in METHODS} for model in MODELS} for task in TASKS} \n",
    "all_candidates1 = {task: {model: {method: [] for method in METHODS} for model in MODELS} for task in TASKS} \n",
    "\n",
    "TASK = [\"task_elections\", \"task_sales\" , \"task_sm\"]\n",
    "TASK = \"task_elections\"\n",
    "TASKS = [TASK]\n",
    "for task in TASKS:\n",
    "    # voters = Voters(bios=bios, task=task, model_name= \"gpt-4o-mini\")\n",
    "    for model in MODELS:\n",
    "        for method in METHODS:\n",
    "            ds = datasets[task][model][method]\n",
    "            cands = ds[\"player_candidates\"]\n",
    "            cands0 = [c[0] for c in cands]\n",
    "            cands1 = [c[1] for c in cands]\n",
    "\n",
    "            all_candidates0[task][model][method] = cands0\n",
    "            all_candidates1[task][model][method] = cands1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voters = Voters(bios=bios, task=task, model_name= \"gpt-4o-mini\")\n",
    "\n",
    "METHOD_PAIRS =[ (\"base\", \"rft\"),  (\"base\", \"tfb\"),  (\"rft\", \"tfb\")]\n",
    "# how superior is the second over the first\n",
    "# 0.5 menas equal, 0.9 menas the second is better, 0.1 menas the first is better\n",
    "# we want to see >0.5\n",
    "\n",
    "# means =  \n",
    "# stds = \n",
    "results = {}\n",
    "results_std = {}\n",
    "for task in TASKS:\n",
    "    results[task] = {}\n",
    "    results_std[task] = {}\n",
    "\n",
    "    voters = Voters(bios=bios, task=task, model_name= \"gpt-4o-mini\")\n",
    "    for model in MODELS:\n",
    "        results[task][model] = {}\n",
    "        results_std[task][model] = {}\n",
    "        for pair in METHOD_PAIRS:\n",
    "            player_candidates = list(zip(all_candidates0[task][model][pair[0]], all_candidates0[task][model][pair[1]]))[:]\n",
    "\n",
    "            voter_votes, voter_thinks, voter_choices = voters.get_votes_list(player_candidates)\n",
    "            mean = np.mean([np.round((Counter(v)[1] / (Counter(v)[1] + Counter(v)[0])), 2).item() for v in voter_votes])\n",
    "            std = np.std([np.round((Counter(v)[1] / (Counter(v)[1] + Counter(v)[0])), 2).item() for v in voter_votes])\n",
    "            \n",
    "            results[task][model][\"-\".join(pair)] = mean\n",
    "            results_std[task][model][\"-\".join(pair)] = std\n",
    "            print(pair, \"Mean:\", mean, \"Std:\", std)\n",
    "\n",
    "import json\n",
    "json_path = f'artsco/res/{TASK}_final.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump({\"mean\": results, \"std\": results_std}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
